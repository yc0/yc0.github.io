<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OpenShift Administration | My journey</title><meta name=keywords content="None"><meta name=description content="Describe OpenShift Container Platform
Overview

An application manages Kubernetes resources: Operator
An application manages Kubernetes operators : Operator Lifecycle Manager(OLM)
A repo. for discovering and installing operators : Operator Catelog
Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators

OpenShift Cluster Version Operator : First-level operator
Cluster Operators are called second-level operators



Summary

Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes
RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring.
Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators.
OperatorHub.io is an online catalog for discovering operators.

Installation
IPI

Full-Stack Installation
Only this way can fulfil cluster scaling
http://try.openshift.org
It does&rsquo;t have to be part of cluster

UPI

User provisioned Infrastructure for pre-existing environment

CoreOS and RHEL

Control Panel must run on CoreOS
Workers can run either CoreOS or RHEL

Installation-config.yaml

the rest of resource domain name following
{metadata.name} + {baseDomain}
network configuration cannot reconfigure easily after cluster is up and running.

Initial deployment process

there&rsquo;s no much customization
UPI mode has to do node certficate when dial to master node (control plane) by manual whereas IPI has no requirment.

Summary

two main installation methods

full-stack automation
pre-existing infrastructures.


OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services
Most of the system run as containers

CRI-O and kubelet


Troubleshooting

oc get node
oc adm top
oc adm node-logs
oc adm debug



Trouble Shooting
oc adm node-log
oc log
oc logs {podname} --all-containers
oc logs {podname} -c {}
oc debug

what went wrong during POD startup
oc debug {pod|deployment} --as-root

oc rsh
$ oc exec -it {podname} -- {command} {--options}

$ oc exec -it {podname} -c {container} -- {command} {--options}
shorter equivalent"><meta name=author content="Nelson"><link rel=canonical href=https://yc0.github.io/posts/openshift-administration/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://yc0.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yc0.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yc0.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://yc0.github.io/apple-touch-icon.png><link rel=mask-icon href=https://yc0.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yc0.github.io/posts/openshift-administration/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yc0.github.io/posts/openshift-administration/"><meta property="og:site_name" content="My journey"><meta property="og:title" content="OpenShift Administration"><meta property="og:description" content="Describe OpenShift Container Platform Overview An application manages Kubernetes resources: Operator An application manages Kubernetes operators : Operator Lifecycle Manager(OLM) A repo. for discovering and installing operators : Operator Catelog Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators OpenShift Cluster Version Operator : First-level operator Cluster Operators are called second-level operators Summary Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring. Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators. OperatorHub.io is an online catalog for discovering operators. Installation IPI Full-Stack Installation Only this way can fulfil cluster scaling http://try.openshift.org It does’t have to be part of cluster UPI User provisioned Infrastructure for pre-existing environment CoreOS and RHEL Control Panel must run on CoreOS Workers can run either CoreOS or RHEL Installation-config.yaml the rest of resource domain name following {metadata.name} + {baseDomain} network configuration cannot reconfigure easily after cluster is up and running. Initial deployment process there’s no much customization UPI mode has to do node certficate when dial to master node (control plane) by manual whereas IPI has no requirment. Summary two main installation methods full-stack automation pre-existing infrastructures. OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services Most of the system run as containers CRI-O and kubelet Troubleshooting oc get node oc adm top oc adm node-logs oc adm debug Trouble Shooting oc adm node-log oc log oc logs {podname} --all-containers oc logs {podname} -c {} oc debug what went wrong during POD startup oc debug {pod|deployment} --as-root oc rsh $ oc exec -it {podname} -- {command} {--options} $ oc exec -it {podname} -c {container} -- {command} {--options} shorter equivalent"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-04-15T23:00:38+00:00"><meta property="article:modified_time" content="2025-10-25T10:47:02+08:00"><meta property="article:tag" content="None"><meta name=twitter:card content="summary"><meta name=twitter:title content="OpenShift Administration"><meta name=twitter:description content="Describe OpenShift Container Platform
Overview

An application manages Kubernetes resources: Operator
An application manages Kubernetes operators : Operator Lifecycle Manager(OLM)
A repo. for discovering and installing operators : Operator Catelog
Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators

OpenShift Cluster Version Operator : First-level operator
Cluster Operators are called second-level operators



Summary

Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes
RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring.
Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators.
OperatorHub.io is an online catalog for discovering operators.

Installation
IPI

Full-Stack Installation
Only this way can fulfil cluster scaling
http://try.openshift.org
It does&rsquo;t have to be part of cluster

UPI

User provisioned Infrastructure for pre-existing environment

CoreOS and RHEL

Control Panel must run on CoreOS
Workers can run either CoreOS or RHEL

Installation-config.yaml

the rest of resource domain name following
{metadata.name} + {baseDomain}
network configuration cannot reconfigure easily after cluster is up and running.

Initial deployment process

there&rsquo;s no much customization
UPI mode has to do node certficate when dial to master node (control plane) by manual whereas IPI has no requirment.

Summary

two main installation methods

full-stack automation
pre-existing infrastructures.


OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services
Most of the system run as containers

CRI-O and kubelet


Troubleshooting

oc get node
oc adm top
oc adm node-logs
oc adm debug



Trouble Shooting
oc adm node-log
oc log
oc logs {podname} --all-containers
oc logs {podname} -c {}
oc debug

what went wrong during POD startup
oc debug {pod|deployment} --as-root

oc rsh
$ oc exec -it {podname} -- {command} {--options}

$ oc exec -it {podname} -c {container} -- {command} {--options}
shorter equivalent"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yc0.github.io/posts/"},{"@type":"ListItem","position":2,"name":"OpenShift Administration","item":"https://yc0.github.io/posts/openshift-administration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OpenShift Administration","name":"OpenShift Administration","description":"Describe OpenShift Container Platform Overview An application manages Kubernetes resources: Operator An application manages Kubernetes operators : Operator Lifecycle Manager(OLM) A repo. for discovering and installing operators : Operator Catelog Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators OpenShift Cluster Version Operator : First-level operator Cluster Operators are called second-level operators Summary Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring. Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators. OperatorHub.io is an online catalog for discovering operators. Installation IPI Full-Stack Installation Only this way can fulfil cluster scaling http://try.openshift.org It does\u0026rsquo;t have to be part of cluster UPI User provisioned Infrastructure for pre-existing environment CoreOS and RHEL Control Panel must run on CoreOS Workers can run either CoreOS or RHEL Installation-config.yaml the rest of resource domain name following {metadata.name} + {baseDomain} network configuration cannot reconfigure easily after cluster is up and running. Initial deployment process there\u0026rsquo;s no much customization UPI mode has to do node certficate when dial to master node (control plane) by manual whereas IPI has no requirment. Summary two main installation methods full-stack automation pre-existing infrastructures. OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services Most of the system run as containers CRI-O and kubelet Troubleshooting oc get node oc adm top oc adm node-logs oc adm debug Trouble Shooting oc adm node-log oc log oc logs {podname} --all-containers oc logs {podname} -c {} oc debug what went wrong during POD startup oc debug {pod|deployment} --as-root oc rsh $ oc exec -it {podname} -- {command} {--options} $ oc exec -it {podname} -c {container} -- {command} {--options} shorter equivalent\n","keywords":["None"],"articleBody":"Describe OpenShift Container Platform Overview An application manages Kubernetes resources: Operator An application manages Kubernetes operators : Operator Lifecycle Manager(OLM) A repo. for discovering and installing operators : Operator Catelog Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators OpenShift Cluster Version Operator : First-level operator Cluster Operators are called second-level operators Summary Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring. Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators. OperatorHub.io is an online catalog for discovering operators. Installation IPI Full-Stack Installation Only this way can fulfil cluster scaling http://try.openshift.org It does’t have to be part of cluster UPI User provisioned Infrastructure for pre-existing environment CoreOS and RHEL Control Panel must run on CoreOS Workers can run either CoreOS or RHEL Installation-config.yaml the rest of resource domain name following {metadata.name} + {baseDomain} network configuration cannot reconfigure easily after cluster is up and running. Initial deployment process there’s no much customization UPI mode has to do node certficate when dial to master node (control plane) by manual whereas IPI has no requirment. Summary two main installation methods full-stack automation pre-existing infrastructures. OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services Most of the system run as containers CRI-O and kubelet Troubleshooting oc get node oc adm top oc adm node-logs oc adm debug Trouble Shooting oc adm node-log oc log oc logs {podname} --all-containers oc logs {podname} -c {} oc debug what went wrong during POD startup oc debug {pod|deployment} --as-root oc rsh $ oc exec -it {podname} -- {command} {--options} $ oc exec -it {podname} -c {container} -- {command} {--options} shorter equivalent\n$ oc rsh {podname} Note why we need – ?\nYou must use two dashes (–) to separate your command’s flags/arguments Identity Providers Concept Describe User Users are entities An actor within the system Interact with the API server Assign permissions by adding roles The user is a member of the group Identity A resource that keeps a record of successful authentication attempts from a specific user and identity provider A single user resource is associated with an identity resource. Service Account Applications can communicate with the API independently Service accounts enable you to control API access with Service Account credentials. Group Groups represent a specific set of users Users are assigned to one or to multiple groups. implementing authorization policies to assign permissions to multiple users at the same time. Role A set of permissions that enables a user to perform API operations over one or more resource types. (Verb + Resources) Summary Creating Users Requires valid credentials managed by an identity provider, user and identity resources\nDeleting Users Deleting their credentials from the identity provider, and also deleting their user and identity resources.\nTwo authentication methods kubeconfig : not recommand, super priviledge. kubeadmin virtual user OAuth Custom Resource HTPasswd Identity Provider htpasswd extract data from secret/store in a secret Assign cluster-admin role to the user to grant a user cluser admin priviledge. Role-based Access Control, RBAC In OpenShift, RBAC determines if a user can perform certain actions within the cluster or project. There’re two types of roles:\nCluster Local. Concept Project V.S. Cluster Actually for rolebinding resource creation\nProject Scope add-role-to-user oc policy add-role-to-user {role} {user} {-n option} it’s better to assign specific namespace to make sure we delegate the designate namespace toward user\nfor example\n$ oc policy add-role-to-user view developer -n test-namespace add-role-to-group remove-role-from-user remove-role-from-group Cluster Scope add-cluster-role-to-user add-cluster-role-to-group remove-cluster-role-from-user remove-cluster-role-from-group Who can oc adm policy who-can {verb} {resource}\nfor example\n$ oc adm policy who-can create projects However, in OpenShift, you cannot directly create project as prioi to mention.\nOpenShift adopts a mechanism projectrequest resource to automatically on the background for making sure the project is created according to certain of settings and\nAdmin Role V.S. Edit Role no role related resources on Edit Role no delete/patch/update permission for projects and namespaces on Edit Role Service Account, SA When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:\nReplication controllers make API calls to create or delete pods Applications inside containers could make API calls for discovery purposes External applications could make API calls for monitoring or integration purposes Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.\nexist within the scope of a project that is to say, if there’re the SAs with the same name, they are totally different objects though. Security Context Constraints, SCC security context constraints, SCC, that control the actions that a pod can perform and what it has the ability to access.\nConcept it evaluates at pod creation time Pod is with the correct SCC, it has to be deleted and be recreated it controls Running privileged containers Requesting extra capabilities to a container Using host directories as volumes Changing the SELinux context of a container Changing the user ID Capabilities Also refer to POSIX capabilities, you can look up piece of information in Linux. The capabilities would be add or remove from the processes running inside the containers\n$ man 7 capabilities Prioitization highest priority first, nil is considered a 0 priority if priority is equal, most restrictive is with high priority Add SCC $ oc adm policy add-scc-to-user {scc} -z {sa_name} -n {namespace} or more straighward\n$ oc adm policy add-scc-to-user {scc} system:serviceaccount:{namespace}:{sa_name} for example\n$ oc adm policy add-scc-to-user noroot system:serviceaccount:troube:privileged Service Account to DeploymentConfig $ oc set serviceaccount deploymentconfig {deployconfig} {service acccount} it can be expressed shorter\n$ oc set sa dc {dc} {sa} Summary Main concept of Role-based access control, RBAC\nSecret resources allow you to separate sensitive information from application pods\nproject scope extract it for extension (configmap as well) Security context constraints (SCCs) t allowed pod interactions with system resources.\nAbbreviation mcs : multiple category security Components DeploymentConfig Concept Networking keypoints : Troubleshoot it and ingress component\nService Kubernetes service IP == Virtual IP Doesn’t allocate any unit/instance A collection of network translation rules 4 types cluster IP node port [older concept] load balancer [older concept] : alway along with cloud providers service name CoreDNS creates two records - A record : resolve FQDN to IP address - SRV record: what port the service uses. - if you use a service exposed TCP 443 through the `https` service in `frontend` namespace - `_443._tcp.https.frontend.svc.cluster.local` Cluster Network Operator can only be configured at installation time\nNetworkType openshiftSDN : 3 types network policy (default) no policy == subnet mode multitenant mode: project-level isolation Pods from different projects cannot communicate with each other unless we create network bridge between them subnet mode (older default, ocp 3.*) flat network Pods can communicate with other Pods whereever their projects Route Ingress in Kubernetes Reverse Proxy When you create route name of service hostname of route Types There’re four types:\nservice edge terminated route re-encrypted route passthrough Summary OpenShift implements a software-defined networking (SDN) to manage the network infrastructure Service allow the logical grouping of pods Service acts as load balancer as well Service selects pods by labels Two route : secure and insecure secure : TLS edge passthrough re-encryption Scheduling OpenShift pod scheduler algorithm Filtering nodes. node condition : disk or memory pressure match labels pod resource demands : CPU, Memory and Storage Taint Prioritizing the filtered nodes Affinity Select a fit node can possible create customerized scheduling policies with policy.cfg key\nLabeling on Node Apart from tranditional way of labelling, CLuster administrator can use -L to determine the single label\n$ oc get node -L failure-domain.beta.kubernetes.io/region Labeling Machine Sets Although node labels are persistent, if your OpenShift cluster contains machine sets (created if your cluster was installed using the full stack automation method, IPI), you should add labels to the MachineSets\nThis ensures that new machines will also contains the desired labels when generating new nodes.\nControlling Pod Placement Infrastructure-related Pods run on master nodes DNS Operator OAuth operator API Server Node Selector $ oc edit deployment/myapp spec: ...output omitted... template: metadata: labels: app: myapp spec: nodeSelector: nev: dev containers: - images: quay.io/redhattraining/scaling:v1.0 ...output omitted... the following command accomplishes the same thing\n$ oc patch deployment/myapp --patch \\ '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"env\":\"dev\"}}}}}' Both commands triggers new deployemnt and new pods scheduled according to the node selector\nDefault Node Selector for a Project default node selector should be configured in the project resource, for example:\ncreate a new project $ oc admin new-project qa --node-selector \"env=qa\" configure on a existing project $ oc annotate namespace qa \\ openshift.io/node-selector=\"env=qa\" --overwrite Scaling $ oc scale --replicas 3 deployment/myapp Limiting Resource Usage Resource Requests Used for scheduling, scheduler find a node with sufficient compute resources. indicate that a pod cannot run with less than the specified amount of compute resources. Resource limits how far Pod can consume the resources prevent a pod from using up all compute resources from a node Linux kernel cgroups feature to enforce the resource limits for the pod. Observation the individual resource comsumption for a pod or the sum amount of the resource comsumption\nBy Describe # oc describe node/{node} $ oc describe node ip-10.10.0.0.us-east-1.compute.internal By Node type # oc adm top node # $ oc adm top node -l node-role.kubernetes.io/worker By project $ oc adm top node -n execute-troubleshoot Quality of Service BestEffort first eviction Burstable second evition Guaranteed never evition unless go node down for maintenence Quota Quotas are limitations on the aggregate consumption of resources of any particular project\nTwo kinds Object counts Compute resources Improve stablility of the OpenShift Avoiding unbounded growth of the Etcd database Avoids exhausting other limited software resources, e.g. IP. Some important Note of Quota Best-Effort Node Any Node with resourcequota cannot accommodate any best-effort node\nMultiple Resource Quota in a project Though we can create multiple resourcequota, we cannot overlap the quota items in the same project\nLimit Ranges give range min limits to max limits give deault limits for containers w/o the requests and limits specified Some Pods, System Pods Deployer pod Builder Pod ClusterQuota project-annotation-selector project-label-selector Lookup project scale\n$ oc describe resourcequota cluster span mulitple project\n$ oc describe appliedclusterresourcequotas due to request quota, there’s not allowed best effort Pods. It brings the situation to setup default - Limit Range.\nScale Manual Scaling $ oc scale deployment/psql --replicas=3 #or $ oc scale dc/psql --replicas=3 the difference between dc and deployment is that dc adopts replicacontroller whereas deployement adopts replicsets. Basically they are with same mechanism\nAutoscaling Concept utilization Matrix subsystem OCP4 pre-installed OCP3 it’s a part of the platform take care of separately Command $ oc autoscale dc/mysql --min=1 --max=5 --cpu-percent=80 Controller by HorizontalPodAutoscalar, HPA Remember that if you want to specify cpu-percent, you must request resources burstable or guaruanteed\nSummary Default pod scheduler spans regions and zones : performance and redundancy.\nPod placement use node selectors w/ labeling nodes.\nResource requests : the minimum amount of resources for scheduling.\nQuotas : restrict the amount consumption of resources a project.\nscales number of replicas of a pod\noc scale for manual oc autoscale for dynamic Scaling an OpenShift Cluster Machine API gives 2 custom resources machine-api operator handle machine-api handle machine two 2 CRD Machineset machine $ oc get clusteroperators $ oc get clusteroperators/machine-api $ oc get -n openshift-machine-api machines $ oc get -n openshift-machine-api machinesets modify Machineset won’t affect existing machines only new machine inherit the features analogous to how ReplicaSet treat pods machine-config operator - handle vm or instance - help it become worker node AutoScaling for Cluster concept Cluster-AutoScaler aggregate all machine-autoscaler\nMachine-AutoScaler Usually for the physical partition purpose, like zone\nPerforming Cluster Updates OpenShift 4 architectural let you update your clusters Over-the-Air (OTA). Red Hat provides a distribution system that ensures the best path for updating your cluster and the underlying operating system. There are two distribution channels: fast and stable fast : delivers updates are soon as they are available stable : delivers delayed updates. Red Hat does not support reverting and only supports upgrading Tips List Project $ oc projects Check CRD $ oc get clusteroperators # oc get clusteroperators/{operator} $ oc get clusteroperators/dns # oc get -o yaml {crd}.operator/{name} $ o get -o yaml dns.operator/default Copy Data # oc cp {source} {pod}:{target} $ oc cp data.sql mysql-5cpd:/tmp/ Check Pod/Deployment Mount # oc set volumes deployment/{name} # oc set volumes pod/{name} $ oc set volumnes deployment/mysql Labelling on Node # add new labels $ oc label node node1.us-east-1.compoute.internal env=dev # modify existing labels $ oc label node node1.us-east-1.compute.internal env=dev --overwrite # remove labels $ oc label node node1.us-east-1.compute.internal env- Control Quota $ oc create quota count -n test \\ --hard=services=5,pods=25,replicationcontrollers=17.... $ oc create quota compute -n test \\ --hard=cpu=5,memory=4Gi,limits.cpu=7,limits.memory=8Gi Retry Schdule $ oc rollout retry dc/{name} $ oc rollout latest dc/{name} Security Context Constraints (SCCs) oc adm policy add-scc-to-user SCC (USER| -z SERVICEACCOUNT) [USER....] ","wordCount":"2199","inLanguage":"en","datePublished":"2020-04-15T23:00:38Z","dateModified":"2025-10-25T10:47:02+08:00","author":{"@type":"Person","name":"Nelson"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yc0.github.io/posts/openshift-administration/"},"publisher":{"@type":"Organization","name":"My journey","logo":{"@type":"ImageObject","url":"https://yc0.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yc0.github.io/ accesskey=h title="My journey (Alt + H)">My journey</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yc0.github.io/posts/ title="All Posts"><span>All Posts</span></a></li><li><a href=https://yc0.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yc0.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yc0.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://yc0.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">OpenShift Administration</h1><div class=post-meta><span title='2020-04-15 23:00:38 +0000 UTC'>April 15, 2020</span>&nbsp;·&nbsp;<span>11 min</span>&nbsp;·&nbsp;<span>Nelson</span></div></header><div class=post-content><h1 id=describe-openshift-container-platform>Describe OpenShift Container Platform<a hidden class=anchor aria-hidden=true href=#describe-openshift-container-platform>#</a></h1><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><ul><li>An application manages Kubernetes resources: Operator</li><li>An application manages Kubernetes operators : Operator Lifecycle Manager(OLM)</li><li>A repo. for discovering and installing operators : Operator Catelog</li><li>Regular operators that are not managed by the OLM. They are managed by OpenShift Cluster Version Operator : Cluster Operators<ul><li>OpenShift Cluster Version Operator : First-level operator</li><li>Cluster Operators are called second-level operators</li></ul></li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li>Red Hat OpenShift Platform is based on Red Hat CoreOS, CRI-O and Kubernetes</li><li>RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container image registry, storage, networking providers, and centralized logging and monitoring.</li><li>Operators package applications that manage Kubernetes resources, and the Operator Lifecycle Manager (OLM) handles installation and management of operators.</li><li>OperatorHub.io is an online catalog for discovering operators.</li></ul><h1 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h1><h2 id=ipi>IPI<a hidden class=anchor aria-hidden=true href=#ipi>#</a></h2><ul><li>Full-Stack Installation</li><li>Only this way can fulfil cluster scaling</li><li><a href=http://try.openshift.org>http://try.openshift.org</a></li><li>It does&rsquo;t have to <strong>be part of cluster</strong></li></ul><h2 id=upi>UPI<a hidden class=anchor aria-hidden=true href=#upi>#</a></h2><ul><li>User provisioned Infrastructure for pre-existing environment</li></ul><h2 id=coreos-and-rhel>CoreOS and RHEL<a hidden class=anchor aria-hidden=true href=#coreos-and-rhel>#</a></h2><ul><li>Control Panel must run on CoreOS</li><li>Workers can run either CoreOS or RHEL</li></ul><h2 id=installation-configyaml>Installation-config.yaml<a hidden class=anchor aria-hidden=true href=#installation-configyaml>#</a></h2><ul><li>the rest of resource domain name following
<code>{metadata.name} + {baseDomain}</code></li><li>network configuration cannot reconfigure easily after cluster is up and running.</li></ul><h2 id=initial-deployment-process>Initial deployment process<a hidden class=anchor aria-hidden=true href=#initial-deployment-process>#</a></h2><ul><li>there&rsquo;s no much customization</li><li>UPI mode has to do node certficate when dial to master node (control plane) <strong>by manual</strong> whereas IPI has no requirment.</li></ul><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><ul><li>two main installation methods<ul><li>full-stack automation</li><li>pre-existing infrastructures.</li></ul></li><li>OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services</li><li>Most of the system run as containers<ul><li>CRI-O and kubelet</li></ul></li><li>Troubleshooting<ul><li><code>oc get node</code></li><li><code>oc adm top</code></li><li><code>oc adm node-logs</code></li><li><code>oc adm debug</code></li></ul></li></ul><h1 id=trouble-shooting>Trouble Shooting<a hidden class=anchor aria-hidden=true href=#trouble-shooting>#</a></h1><h2 id=oc-adm-node-log>oc adm node-log<a hidden class=anchor aria-hidden=true href=#oc-adm-node-log>#</a></h2><h2 id=oc-log>oc log<a hidden class=anchor aria-hidden=true href=#oc-log>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>oc logs <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span> --all-containers
</span></span><span style=display:flex><span>oc logs <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span> -c <span style=color:#f92672>{}</span>
</span></span></code></pre></div><h2 id=oc-debug>oc debug<a hidden class=anchor aria-hidden=true href=#oc-debug>#</a></h2><ul><li>what went wrong during POD startup</li><li><code>oc debug {pod|deployment} --as-root</code></li></ul><h2 id=oc-rsh>oc rsh<a hidden class=anchor aria-hidden=true href=#oc-rsh>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc exec -it <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span> -- <span style=color:#f92672>{</span>command<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--options<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ oc exec -it <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span> -c <span style=color:#f92672>{</span>container<span style=color:#f92672>}</span> -- <span style=color:#f92672>{</span>command<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>--options<span style=color:#f92672>}</span>
</span></span></code></pre></div><p>shorter equivalent</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc rsh <span style=color:#f92672>{</span>podname<span style=color:#f92672>}</span>
</span></span></code></pre></div><h3 id=note>Note<a hidden class=anchor aria-hidden=true href=#note>#</a></h3><p>why we need &ndash; ?</p><ul><li>You must use two dashes (&ndash;) to separate your command&rsquo;s flags/arguments</li></ul><h1 id=identity-providers>Identity Providers<a hidden class=anchor aria-hidden=true href=#identity-providers>#</a></h1><h2 id=concept>Concept<a hidden class=anchor aria-hidden=true href=#concept>#</a></h2><img loading=lazy src=https://user-images.githubusercontent.com/10542832/79710264-10496580-82f7-11ea-9b09-2d000623a33b.png><h2 id=describe>Describe<a hidden class=anchor aria-hidden=true href=#describe>#</a></h2><h3 id=user>User<a hidden class=anchor aria-hidden=true href=#user>#</a></h3><ul><li>Users are entities</li><li>An actor within the system</li><li>Interact with the API server</li><li>Assign permissions by adding roles</li><li>The user is a member of the group</li></ul><h3 id=identity>Identity<a hidden class=anchor aria-hidden=true href=#identity>#</a></h3><ul><li>A resource that keeps <strong>a record</strong> of <strong>successful authentication</strong> attempts from a specific user and identity provider</li><li>A single user resource is associated with an identity resource.</li></ul><h3 id=service-account>Service Account<a hidden class=anchor aria-hidden=true href=#service-account>#</a></h3><ul><li>Applications can communicate with the API independently</li><li>Service accounts enable you to control API access with Service Account credentials.</li></ul><h3 id=group>Group<a hidden class=anchor aria-hidden=true href=#group>#</a></h3><ul><li>Groups represent a specific set of users</li><li>Users are assigned to one or to multiple groups.</li><li>implementing authorization policies to assign permissions <strong>to multiple users at the same time.</strong></li></ul><h3 id=role>Role<a hidden class=anchor aria-hidden=true href=#role>#</a></h3><ul><li>A set of permissions that enables a user to perform API operations over one or more resource types. <code>(Verb + Resources)</code></li></ul><h2 id=summary-2>Summary<a hidden class=anchor aria-hidden=true href=#summary-2>#</a></h2><h3 id=creating-users>Creating Users<a hidden class=anchor aria-hidden=true href=#creating-users>#</a></h3><p>Requires valid credentials managed by an identity provider, user and identity resources</p><h3 id=deleting-users>Deleting Users<a hidden class=anchor aria-hidden=true href=#deleting-users>#</a></h3><p>Deleting their credentials from the identity provider, and also deleting their user and identity resources.</p><h3 id=two-authentication-methods>Two authentication methods<a hidden class=anchor aria-hidden=true href=#two-authentication-methods>#</a></h3><ul><li>kubeconfig : not recommand, super priviledge.</li><li>kubeadmin virtual user</li></ul><h3 id=oauth-custom-resource>OAuth Custom Resource<a hidden class=anchor aria-hidden=true href=#oauth-custom-resource>#</a></h3><ul><li>HTPasswd Identity Provider<ul><li>htpasswd</li><li>extract data from secret/store in a secret</li></ul></li><li>Assign <code>cluster-admin</code> role to the user to grant a user cluser admin priviledge.</li></ul><h1 id=role-based-access-control-rbac>Role-based Access Control, RBAC<a hidden class=anchor aria-hidden=true href=#role-based-access-control-rbac>#</a></h1><p>In OpenShift, RBAC determines if a user can perform certain actions within the cluster or project. There&rsquo;re two types of roles:</p><ul><li>Cluster</li><li>Local.</li></ul><h2 id=concept-1>Concept<a hidden class=anchor aria-hidden=true href=#concept-1>#</a></h2><img alt=rbac loading=lazy src=https://user-images.githubusercontent.com/10542832/79726062-c96e6680-831c-11ea-9129-9fc372ce6b13.png><h2 id=project-vs-cluster>Project V.S. Cluster<a hidden class=anchor aria-hidden=true href=#project-vs-cluster>#</a></h2><p>Actually for rolebinding resource creation</p><h3 id=project-scope>Project Scope<a hidden class=anchor aria-hidden=true href=#project-scope>#</a></h3><ul><li>add-role-to-user
<code>oc policy add-role-to-user {role} {user} {-n option}</code></li></ul><p>it&rsquo;s better to assign specific namespace to make sure we delegate the designate namespace toward user</p><p>for example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc policy add-role-to-user view developer -n test-namespace
</span></span></code></pre></div><ul><li>add-role-to-group</li><li>remove-role-from-user</li><li>remove-role-from-group</li></ul><h3 id=cluster-scope>Cluster Scope<a hidden class=anchor aria-hidden=true href=#cluster-scope>#</a></h3><ul><li>add-cluster-role-to-user</li><li>add-cluster-role-to-group</li><li>remove-cluster-role-from-user</li><li>remove-cluster-role-from-group</li></ul><h2 id=who-can>Who can<a hidden class=anchor aria-hidden=true href=#who-can>#</a></h2><p><code>oc adm policy who-can {verb} {resource}</code></p><p>for example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc adm policy who-can create projects
</span></span></code></pre></div><p>However, in OpenShift, you cannot directly <code>create project</code> as prioi to mention.</p><p>OpenShift adopts a mechanism <code>projectrequest</code> resource to automatically on the background for making sure <strong>the project</strong> is created according to certain of settings and</p><h2 id=admin-role-vs-edit-role>Admin Role V.S. Edit Role<a hidden class=anchor aria-hidden=true href=#admin-role-vs-edit-role>#</a></h2><ul><li>no role related resources on Edit Role</li><li>no delete/patch/update permission for projects and namespaces on Edit Role</li></ul><h2 id=service-account-sa>Service Account, SA<a hidden class=anchor aria-hidden=true href=#service-account-sa>#</a></h2><p>When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:</p><p>Replication controllers make API calls to create or delete pods
Applications inside containers could make API calls for discovery purposes
External applications could make API calls for monitoring or integration purposes
Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.</p><ul><li>exist within <strong>the scope of a project</strong><ul><li>that is to say, if there&rsquo;re the SAs with the same name, they are totally different objects though.</li></ul></li></ul><h1 id=security-context-constraints-scc>Security Context Constraints, SCC<a hidden class=anchor aria-hidden=true href=#security-context-constraints-scc>#</a></h1><p>security context constraints, SCC, that control the actions that a pod can perform and what it has the ability to access.</p><h2 id=concept-2>Concept<a hidden class=anchor aria-hidden=true href=#concept-2>#</a></h2><ul><li>it evaluates <strong>at pod creation time</strong><ul><li>Pod is with the correct SCC, it has to be deleted and be recreated</li></ul></li><li>it controls<ul><li>Running privileged containers</li><li>Requesting extra capabilities to a container</li><li>Using host directories as volumes</li><li>Changing the SELinux context of a container</li><li>Changing the user ID</li></ul></li></ul><h2 id=capabilities>Capabilities<a hidden class=anchor aria-hidden=true href=#capabilities>#</a></h2><p>Also refer to <strong>POSIX capabilities</strong>, you can look up piece of information in Linux.
The capabilities would be add or remove from the processes running inside the containers</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ man <span style=color:#ae81ff>7</span> capabilities
</span></span></code></pre></div><h2 id=prioitization>Prioitization<a hidden class=anchor aria-hidden=true href=#prioitization>#</a></h2><ol><li>highest priority first, nil is considered a 0 priority</li><li>if priority is equal, most restrictive is with high priority</li></ol><h2 id=add-scc>Add SCC<a hidden class=anchor aria-hidden=true href=#add-scc>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc adm policy add-scc-to-user <span style=color:#f92672>{</span>scc<span style=color:#f92672>}</span> -z <span style=color:#f92672>{</span>sa_name<span style=color:#f92672>}</span> -n <span style=color:#f92672>{</span>namespace<span style=color:#f92672>}</span> 
</span></span></code></pre></div><p>or more straighward</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc adm policy add-scc-to-user <span style=color:#f92672>{</span>scc<span style=color:#f92672>}</span> system:serviceaccount:<span style=color:#f92672>{</span>namespace<span style=color:#f92672>}</span>:<span style=color:#f92672>{</span>sa_name<span style=color:#f92672>}</span>
</span></span></code></pre></div><p>for example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc adm policy add-scc-to-user noroot system:serviceaccount:troube:privileged
</span></span></code></pre></div><h2 id=service-account-to-deploymentconfig>Service Account to DeploymentConfig<a hidden class=anchor aria-hidden=true href=#service-account-to-deploymentconfig>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc set serviceaccount deploymentconfig <span style=color:#f92672>{</span>deployconfig<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>service acccount<span style=color:#f92672>}</span>
</span></span></code></pre></div><p>it can be expressed shorter</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc set sa dc <span style=color:#f92672>{</span>dc<span style=color:#f92672>}</span> <span style=color:#f92672>{</span>sa<span style=color:#f92672>}</span>
</span></span></code></pre></div><h2 id=summary-3>Summary<a hidden class=anchor aria-hidden=true href=#summary-3>#</a></h2><ul><li><p><img loading=lazy src=https://user-images.githubusercontent.com/10542832/79726062-c96e6680-831c-11ea-9129-9fc372ce6b13.png>
Main concept of Role-based access control, RBAC</p></li><li><p>Secret resources allow you to separate sensitive information from application pods</p><ul><li>project scope</li><li>extract it for extension (configmap as well)</li></ul></li><li><p>Security context constraints (SCCs) t allowed pod interactions with system resources.</p></li></ul><h2 id=abbreviation>Abbreviation<a hidden class=anchor aria-hidden=true href=#abbreviation>#</a></h2><ul><li>mcs : multiple category security</li></ul><h1 id=components>Components<a hidden class=anchor aria-hidden=true href=#components>#</a></h1><h2 id=deploymentconfig>DeploymentConfig<a hidden class=anchor aria-hidden=true href=#deploymentconfig>#</a></h2><h3 id=concept-3>Concept<a hidden class=anchor aria-hidden=true href=#concept-3>#</a></h3><img loading=lazy src=https://user-images.githubusercontent.com/10542832/79877134-711f8d80-841e-11ea-859c-cb514d4d7747.png><h2 id=networking>Networking<a hidden class=anchor aria-hidden=true href=#networking>#</a></h2><p>keypoints : <strong>Troubleshoot it</strong> and <strong>ingress component</strong></p><h3 id=service>Service<a hidden class=anchor aria-hidden=true href=#service>#</a></h3><ul><li>Kubernetes service IP == Virtual IP</li><li>Doesn&rsquo;t allocate any unit/instance</li><li>A <strong>collection</strong> of network translation rules</li><li>4 types<ul><li>cluster IP</li><li>node port [older concept]</li><li>load balancer [older concept] : alway along with cloud providers</li><li>service name
<img loading=lazy src=https://user-images.githubusercontent.com/10542832/80056079-ee015300-8555-11ea-886b-85f6d7e0ba0d.png></li></ul></li></ul><h3 id=coredns-creates-two-records>CoreDNS creates two records<a hidden class=anchor aria-hidden=true href=#coredns-creates-two-records>#</a></h3><pre><code>- A record : resolve FQDN to IP address
- SRV record: what port the service uses.
    - if you use a service exposed TCP 443 through the `https` service in `frontend` namespace
    - `_443._tcp.https.frontend.svc.cluster.local`
</code></pre><h3 id=cluster-network-operator>Cluster Network Operator<a hidden class=anchor aria-hidden=true href=#cluster-network-operator>#</a></h3><p>can only be configured at installation time</p><h4 id=networktype>NetworkType<a hidden class=anchor aria-hidden=true href=#networktype>#</a></h4><ul><li>openshiftSDN : 3 types<ul><li>network policy (default)<ul><li>no policy == subnet mode</li></ul></li><li>multitenant mode: project-level isolation<ul><li>Pods from different projects cannot communicate with each other</li><li>unless we create network bridge between them</li></ul></li><li>subnet mode (older default, ocp 3.*)<ul><li>flat network</li><li>Pods can communicate with other Pods whereever their projects</li></ul></li></ul></li></ul><h3 id=route>Route<a hidden class=anchor aria-hidden=true href=#route>#</a></h3><ul><li>Ingress in Kubernetes</li><li>Reverse Proxy</li><li>When you create <code>route</code><ul><li>name of service</li><li>hostname of route</li></ul></li></ul><h4 id=types>Types<a hidden class=anchor aria-hidden=true href=#types>#</a></h4><p>There&rsquo;re four types:</p><ul><li>service</li><li>edge terminated route</li><li>re-encrypted route</li><li>passthrough</li></ul><img alt="router type" loading=lazy src=https://user-images.githubusercontent.com/10542832/80170176-f8852080-8619-11ea-97b9-04a578176c84.png><h3 id=summary-4>Summary<a hidden class=anchor aria-hidden=true href=#summary-4>#</a></h3><ul><li>OpenShift implements a software-defined networking (SDN) to manage the network infrastructure</li><li><code>Service</code> allow the logical grouping of pods</li><li><code>Service</code> acts as load balancer as well</li><li><code>Service</code> selects pods by labels</li><li>Two route : <code>secure</code> and <code>insecure</code><ul><li>secure : TLS<ul><li>edge</li><li>passthrough</li><li>re-encryption</li></ul></li></ul></li></ul><h1 id=scheduling>Scheduling<a hidden class=anchor aria-hidden=true href=#scheduling>#</a></h1><h2 id=openshift-pod-scheduler-algorithm>OpenShift pod scheduler algorithm<a hidden class=anchor aria-hidden=true href=#openshift-pod-scheduler-algorithm>#</a></h2><ol><li>Filtering nodes.<ul><li>node condition : disk or memory pressure</li><li>match labels</li><li>pod resource demands : CPU, Memory and Storage</li><li>Taint</li></ul></li><li>Prioritizing the filtered nodes<ul><li>Affinity</li></ul></li><li>Select a fit node</li></ol><p>can possible create customerized scheduling policies with <code>policy.cfg</code> key</p><h2 id=labeling-on-node>Labeling on Node<a hidden class=anchor aria-hidden=true href=#labeling-on-node>#</a></h2><p>Apart from tranditional way of labelling,
CLuster administrator can use <code>-L</code> to determine the single label</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc get node -L failure-domain.beta.kubernetes.io/region
</span></span></code></pre></div><h2 id=labeling-machine-sets>Labeling Machine Sets<a hidden class=anchor aria-hidden=true href=#labeling-machine-sets>#</a></h2><p>Although node labels are persistent, if your OpenShift cluster contains machine sets (created if your cluster was installed using the full stack automation method, IPI), you should add labels to the <code>MachineSets</code></p><p>This ensures that new machines will also contains the desired labels when generating new nodes.</p><h2 id=controlling-pod-placement>Controlling Pod Placement<a hidden class=anchor aria-hidden=true href=#controlling-pod-placement>#</a></h2><h3 id=infrastructure-related-pods>Infrastructure-related Pods<a hidden class=anchor aria-hidden=true href=#infrastructure-related-pods>#</a></h3><ul><li>run on master nodes</li><li>DNS Operator</li><li>OAuth operator</li><li>API Server</li></ul><h3 id=node-selector>Node Selector<a hidden class=anchor aria-hidden=true href=#node-selector>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc edit deployment/myapp
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>...<span style=color:#ae81ff>output omitted...</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>myapp</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodeSelector</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>nev</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>images</span>: <span style=color:#ae81ff>quay.io/redhattraining/scaling:v1.0</span>
</span></span><span style=display:flex><span>...<span style=color:#ae81ff>output omitted...</span>
</span></span></code></pre></div><p>the following command accomplishes the same thing</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc patch deployment/myapp --patch <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span><span style=color:#e6db74>&#39;{&#34;spec&#34;:{&#34;template&#34;:{&#34;spec&#34;:{&#34;nodeSelector&#34;:{&#34;env&#34;:&#34;dev&#34;}}}}}&#39;</span>
</span></span></code></pre></div><p>Both commands triggers new deployemnt and new pods scheduled according to the node selector</p><h2 id=default-node-selector-for-a-project>Default Node Selector for a Project<a hidden class=anchor aria-hidden=true href=#default-node-selector-for-a-project>#</a></h2><p>default node selector should be configured in the project resource, for example:</p><h3 id=create-a-new-project>create a new project<a hidden class=anchor aria-hidden=true href=#create-a-new-project>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc admin new-project qa --node-selector <span style=color:#e6db74>&#34;env=qa&#34;</span>
</span></span></code></pre></div><h3 id=configure-on-a-existing-project>configure on a existing project<a hidden class=anchor aria-hidden=true href=#configure-on-a-existing-project>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc annotate namespace qa <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>openshift.io/node-selector<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;env=qa&#34;</span> --overwrite
</span></span></code></pre></div><h2 id=scaling>Scaling<a hidden class=anchor aria-hidden=true href=#scaling>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc scale --replicas <span style=color:#ae81ff>3</span> deployment/myapp
</span></span></code></pre></div><h1 id=limiting-resource-usage>Limiting Resource Usage<a hidden class=anchor aria-hidden=true href=#limiting-resource-usage>#</a></h1><h2 id=resource-requests>Resource Requests<a hidden class=anchor aria-hidden=true href=#resource-requests>#</a></h2><ul><li>Used for scheduling, scheduler find a node with sufficient compute resources.</li><li>indicate that a pod cannot run with less than the specified amount of compute resources.</li></ul><h2 id=resource-limits>Resource limits<a hidden class=anchor aria-hidden=true href=#resource-limits>#</a></h2><ul><li>how far Pod can consume the resources</li><li>prevent a pod from using up all compute resources from a node</li><li>Linux kernel cgroups feature to enforce the resource limits for the pod.</li></ul><h2 id=observation>Observation<a hidden class=anchor aria-hidden=true href=#observation>#</a></h2><p>the individual resource comsumption for a pod or the sum amount of the resource comsumption</p><h3 id=by-describe>By Describe<a hidden class=anchor aria-hidden=true href=#by-describe>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># oc describe node/{node}</span>
</span></span><span style=display:flex><span>$ oc describe node ip-10.10.0.0.us-east-1.compute.internal 
</span></span></code></pre></div><h3 id=by-node-type>By Node type<a hidden class=anchor aria-hidden=true href=#by-node-type>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># oc adm top node</span>
</span></span><span style=display:flex><span><span style=color:#75715e># </span>
</span></span><span style=display:flex><span>$ oc adm top node -l node-role.kubernetes.io/worker
</span></span></code></pre></div><h3 id=by-project>By project<a hidden class=anchor aria-hidden=true href=#by-project>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc adm top node -n execute-troubleshoot
</span></span></code></pre></div><h2 id=quality-of-service>Quality of Service<a hidden class=anchor aria-hidden=true href=#quality-of-service>#</a></h2><ol><li>BestEffort<ul><li>first eviction</li></ul></li><li>Burstable<ul><li>second evition</li></ul></li><li>Guaranteed<ul><li>never evition</li><li>unless go node down for maintenence</li></ul></li></ol><h2 id=quota>Quota<a hidden class=anchor aria-hidden=true href=#quota>#</a></h2><p>Quotas are limitations on the aggregate consumption of resources of any particular project</p><h3 id=two-kinds>Two kinds<a hidden class=anchor aria-hidden=true href=#two-kinds>#</a></h3><ul><li>Object counts</li><li>Compute resources</li></ul><h3 id=improve-stablility-of-the-openshift>Improve stablility of the OpenShift<a hidden class=anchor aria-hidden=true href=#improve-stablility-of-the-openshift>#</a></h3><ul><li>Avoiding unbounded growth of the Etcd database</li><li>Avoids exhausting other limited software resources, e.g. IP.</li></ul><h2 id=some-important-note-of-quota>Some important Note of Quota<a hidden class=anchor aria-hidden=true href=#some-important-note-of-quota>#</a></h2><h3 id=best-effort-node>Best-Effort Node<a hidden class=anchor aria-hidden=true href=#best-effort-node>#</a></h3><p>Any Node with resourcequota cannot accommodate any <code>best-effort</code> node</p><h3 id=multiple-resource-quota-in-a-project>Multiple Resource Quota in a project<a hidden class=anchor aria-hidden=true href=#multiple-resource-quota-in-a-project>#</a></h3><p>Though we can create multiple <code>resourcequota</code>, we cannot overlap the quota items in the same project</p><h2 id=limit-ranges>Limit Ranges<a hidden class=anchor aria-hidden=true href=#limit-ranges>#</a></h2><ul><li>give range min limits to max limits</li><li>give deault limits for containers w/o the requests and limits specified</li></ul><h3 id=some-pods-system-pods>Some Pods, System Pods<a hidden class=anchor aria-hidden=true href=#some-pods-system-pods>#</a></h3><ul><li>Deployer pod</li><li>Builder Pod</li></ul><h2 id=clusterquota>ClusterQuota<a hidden class=anchor aria-hidden=true href=#clusterquota>#</a></h2><ul><li>project-annotation-selector</li><li>project-label-selector</li></ul><h2 id=lookup>Lookup<a hidden class=anchor aria-hidden=true href=#lookup>#</a></h2><p>project scale</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc describe resourcequota
</span></span></code></pre></div><p>cluster span mulitple project</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc describe appliedclusterresourcequotas
</span></span></code></pre></div><p>due to <code>request quota</code>, there&rsquo;s not allowed <code>best effort</code> Pods. It brings the situation to setup default - <code>Limit Range</code>.</p><h1 id=scale>Scale<a hidden class=anchor aria-hidden=true href=#scale>#</a></h1><h2 id=manual-scaling>Manual Scaling<a hidden class=anchor aria-hidden=true href=#manual-scaling>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc scale deployment/psql --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#or</span>
</span></span><span style=display:flex><span>$ oc scale dc/psql --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>
</span></span></code></pre></div><p>the difference between dc and deployment is that dc adopts replicacontroller whereas deployement adopts replicsets. Basically they are with same mechanism</p><h2 id=autoscaling>Autoscaling<a hidden class=anchor aria-hidden=true href=#autoscaling>#</a></h2><h3 id=concept-4>Concept<a hidden class=anchor aria-hidden=true href=#concept-4>#</a></h3><img alt=autoscaling loading=lazy src=https://user-images.githubusercontent.com/10542832/80395043-45e8e280-88e5-11ea-9025-5fd6a97f24bd.png><h3 id=utilization>utilization<a hidden class=anchor aria-hidden=true href=#utilization>#</a></h3><img alt=utilization loading=lazy src=https://user-images.githubusercontent.com/10542832/80303941-cfb18680-87e5-11ea-9c32-9caeacfb3336.png><h3 id=matrix-subsystem>Matrix subsystem<a hidden class=anchor aria-hidden=true href=#matrix-subsystem>#</a></h3><ul><li>OCP4 pre-installed</li><li>OCP3 it&rsquo;s a part of the platform<ul><li>take care of separately</li></ul></li></ul><h3 id=command>Command<a hidden class=anchor aria-hidden=true href=#command>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc autoscale dc/mysql --min<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> --max<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span> --cpu-percent<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h3 id=controller-by-horizontalpodautoscalar-hpa>Controller by HorizontalPodAutoscalar, HPA<a hidden class=anchor aria-hidden=true href=#controller-by-horizontalpodautoscalar-hpa>#</a></h3><p>Remember that if you want to specify cpu-percent, you must request resources <code>burstable or guaruanteed</code></p><h2 id=summary-5>Summary<a hidden class=anchor aria-hidden=true href=#summary-5>#</a></h2><ul><li><p>Default pod scheduler spans regions and zones : performance and redundancy.</p></li><li><p>Pod placement use node selectors w/ labeling nodes.</p></li><li><p>Resource requests : the minimum amount of resources for scheduling.</p></li><li><p>Quotas : restrict the amount consumption of resources a project.</p></li><li><p>scales number of replicas of a pod</p><ul><li><code>oc scale</code> for manual</li><li><code>oc autoscale</code> for dynamic</li></ul></li></ul><h1 id=scaling-an-openshift-cluster>Scaling an OpenShift Cluster<a hidden class=anchor aria-hidden=true href=#scaling-an-openshift-cluster>#</a></h1><h2 id=machine-api-gives-2-custom-resources>Machine API gives 2 custom resources<a hidden class=anchor aria-hidden=true href=#machine-api-gives-2-custom-resources>#</a></h2><h3 id=machine-api-operator><code>machine-api</code> operator<a hidden class=anchor aria-hidden=true href=#machine-api-operator>#</a></h3><ul><li>handle machine-api</li><li>handle machine</li><li>two 2 CRD<ul><li>Machineset</li><li>machine</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc get clusteroperators
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ oc get clusteroperators/machine-api
</span></span><span style=display:flex><span>$ oc get -n openshift-machine-api machines
</span></span><span style=display:flex><span>$ oc get -n openshift-machine-api machinesets
</span></span></code></pre></div><ul><li>modify Machineset<ul><li>won&rsquo;t affect existing machines</li><li>only new machine inherit the features</li><li>analogous to how <code>ReplicaSet</code> treat pods</li></ul></li></ul><h3 id=machine-config-operator><code>machine-config</code> operator<a hidden class=anchor aria-hidden=true href=#machine-config-operator>#</a></h3><pre><code>- handle vm or instance
- help it become worker node
</code></pre><h2 id=autoscaling-for-cluster>AutoScaling for Cluster<a hidden class=anchor aria-hidden=true href=#autoscaling-for-cluster>#</a></h2><h3 id=concept-5>concept<a hidden class=anchor aria-hidden=true href=#concept-5>#</a></h3><img alt=cluster-autoscaler loading=lazy src=https://user-images.githubusercontent.com/10542832/80396081-b6dcca00-88e6-11ea-8d7e-9f3e3068ed28.png><h3 id=cluster-autoscaler>Cluster-AutoScaler<a hidden class=anchor aria-hidden=true href=#cluster-autoscaler>#</a></h3><p>aggregate all machine-autoscaler</p><h3 id=machine-autoscaler>Machine-AutoScaler<a hidden class=anchor aria-hidden=true href=#machine-autoscaler>#</a></h3><p>Usually for the physical partition purpose, like zone</p><h1 id=performing-cluster-updates>Performing Cluster Updates<a hidden class=anchor aria-hidden=true href=#performing-cluster-updates>#</a></h1><ul><li>OpenShift 4 architectural let you update your clusters <strong>Over-the-Air</strong> (OTA).</li><li>Red Hat provides a distribution system that ensures the best path for updating your cluster and the underlying operating system.</li><li>There are two distribution channels: fast and stable<ul><li>fast : delivers updates are soon as they are available</li><li>stable : delivers delayed updates.</li></ul></li><li>Red Hat does not support reverting and only supports upgrading</li></ul><h1 id=tips>Tips<a hidden class=anchor aria-hidden=true href=#tips>#</a></h1><h2 id=list-project>List Project<a hidden class=anchor aria-hidden=true href=#list-project>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc projects
</span></span></code></pre></div><h2 id=check-crd>Check CRD<a hidden class=anchor aria-hidden=true href=#check-crd>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc get clusteroperators
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># oc get clusteroperators/{operator}</span>
</span></span><span style=display:flex><span>$ oc get clusteroperators/dns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># oc get -o yaml {crd}.operator/{name}</span>
</span></span><span style=display:flex><span>$ o get -o yaml dns.operator/default
</span></span></code></pre></div><h2 id=copy-data>Copy Data<a hidden class=anchor aria-hidden=true href=#copy-data>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># oc cp {source} {pod}:{target}</span>
</span></span><span style=display:flex><span>$ oc cp data.sql mysql-5cpd:/tmp/
</span></span></code></pre></div><h2 id=check-poddeployment-mount>Check Pod/Deployment Mount<a hidden class=anchor aria-hidden=true href=#check-poddeployment-mount>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># oc set volumes deployment/{name}</span>
</span></span><span style=display:flex><span><span style=color:#75715e># oc set volumes pod/{name}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ oc set volumnes deployment/mysql
</span></span></code></pre></div><h2 id=labelling-on-node>Labelling on Node<a hidden class=anchor aria-hidden=true href=#labelling-on-node>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># add new labels</span>
</span></span><span style=display:flex><span>$ oc label node node1.us-east-1.compoute.internal env<span style=color:#f92672>=</span>dev  
</span></span><span style=display:flex><span><span style=color:#75715e># modify existing labels</span>
</span></span><span style=display:flex><span>$ oc label node node1.us-east-1.compute.internal env<span style=color:#f92672>=</span>dev --overwrite
</span></span><span style=display:flex><span><span style=color:#75715e># remove labels</span>
</span></span><span style=display:flex><span>$ oc label node node1.us-east-1.compute.internal env-
</span></span></code></pre></div><h2 id=control-quota>Control Quota<a hidden class=anchor aria-hidden=true href=#control-quota>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc create quota count -n test <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span> --hard<span style=color:#f92672>=</span>services<span style=color:#f92672>=</span>5,pods<span style=color:#f92672>=</span>25,replicationcontrollers<span style=color:#f92672>=</span>17....
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ oc create quota compute -n test <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span> --hard<span style=color:#f92672>=</span>cpu<span style=color:#f92672>=</span>5,memory<span style=color:#f92672>=</span>4Gi,limits.cpu<span style=color:#f92672>=</span>7,limits.memory<span style=color:#f92672>=</span>8Gi
</span></span></code></pre></div><h2 id=retry-schdule>Retry Schdule<a hidden class=anchor aria-hidden=true href=#retry-schdule>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ oc rollout retry dc/<span style=color:#f92672>{</span>name<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>$ oc rollout latest dc/<span style=color:#f92672>{</span>name<span style=color:#f92672>}</span>
</span></span></code></pre></div><h2 id=security-context-constraints-sccs>Security Context Constraints (SCCs)<a hidden class=anchor aria-hidden=true href=#security-context-constraints-sccs>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>oc adm policy add-scc-to-user SCC <span style=color:#f92672>(</span>USER| -z SERVICEACCOUNT<span style=color:#f92672>)</span> <span style=color:#f92672>[</span>USER....<span style=color:#f92672>]</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://yc0.github.io/tags/none/>None</a></li></ul><nav class=paginav><a class=prev href=https://yc0.github.io/posts/sar-linux-tools/><span class=title>« Prev</span><br><span>SAR Linux Tools</span>
</a><a class=next href=https://yc0.github.io/posts/istio-overview/><span class=title>Next »</span><br><span>Istio Overview</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yc0.github.io/>My journey</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>