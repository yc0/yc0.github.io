---
date: '2025-11-23T00:00:00+08:00'
draft: true
categories:
  - consistent hashing
  - system design
title: File Storage Serviceï¼ˆDropbox é¡ç³»çµ±ï¼‰
---
# Dropbox / Google Drive --- **Format 3C å®Œæ•´ç‰ˆ**

## ï¼ˆHybridï¼šå£èªè¬›ç¨¿ + Technical Deep Dive + Multi-Region Syncï¼‰
---
# ğŸ¤ 0. Openingï¼ˆé¢è©¦é–‹å ´ 45--60 ç§’ï¼‰

æˆ‘æœƒæŠŠ Dropbox / Google Drive é¡ç³»çµ±åˆ†æˆä¸‰å±¤é€²è¡Œèªªæ˜ï¼š

1.  **å–® Region baseline æ¶æ§‹**ï¼šmetadata + object storage + sync model\
2.  **Scalability æ·±åº¦è§£æ**ï¼šchunkingã€dedupã€metadata shardingã€change
    journal\
3.  **Multi-region global sync**ï¼šprimary regionã€cross-region
    syncã€conflict handling

æ•´å€‹ç³»çµ±çš„æ ¸å¿ƒä¸æ˜¯å„²å­˜æª”æ¡ˆï¼Œè€Œæ˜¯**å¦‚ä½•åšåˆ°æ­£ç¢ºã€é«˜æ•ˆç‡ã€å¯æ¢å¾©çš„åŒæ­¥æ©Ÿåˆ¶ï¼ˆsynchronization
modelï¼‰**ã€‚

------------------------------------------------------------------------

# 1. å•é¡Œå®šç¾©èˆ‡ Use Cases

ä½¿ç”¨è€…éœ€æ±‚ï¼š

-   å¤šè£ç½®åŒæ­¥æª”æ¡ˆï¼ˆæ‰‹æ©Ÿ / PC / å¹³æ¿ï¼‰\
-   å–®ä¸€è£ç½®ç·¨è¼¯æª”æ¡ˆ â†’ æ‰€æœ‰è£ç½®å¿«é€ŸåŒæ­¥\
-   æ”¯æ´ offline editing\
-   æ”¯æ´ conflict resolve\
-   æª”æ¡ˆç‰ˆæœ¬ç®¡ç†\
-   æ”¯æ´å¤§é‡å°æª”æ¡ˆã€å¤§æª”æ¡ˆï¼ˆ\>1GBï¼‰

ç³»çµ±éœ€ä¿æŒï¼š

-   **é«˜ durabilityï¼ˆä¸èƒ½éºå¤±è³‡æ–™ï¼‰**\
-   **å¯é æ¸¬ã€å¼·ä¸€è‡´çš„ metadata**\
-   **é«˜ QPS çš„ sync event æ¨æ’­èƒ½åŠ›**

------------------------------------------------------------------------

# 2. åŠŸèƒ½æ€§éœ€æ±‚ï¼ˆFunctional Requirementsï¼‰

æ ¸å¿ƒï¼š

1.  Upload / Download\
2.  Multi-device sync\
3.  Versioning\
4.  Conflict resolution\
5.  Sharing\
6.  Directory managementï¼ˆrename, move, deleteï¼‰

é€²éšï¼š

7.  Selective sync\
8.  LAN sync\
9.  Deep deduplication\
10. File preview / thumbnail service

------------------------------------------------------------------------

# 3. éåŠŸèƒ½æ€§éœ€æ±‚ + Capacity & Performance ä¼°ç®—

## 3.1 Performance Targets

  è¡Œç‚º                     å»¶é²
  ------------------------ --------------
  metadata update          P95 \< 150ms
  sync event propagation   \< 5 ç§’
  chunk upload             å¯å®¹å¿æ•¸ç§’

## 3.2 QPS & Order-of-Magnitude Estimation

å‡è¨­ï¼š

-   50M daily active users\
-   æ¯äººæ¯å¤©å¹³å‡ 40 å€‹ sync events

å‰‡ï¼š

    2B sync events / day â‰ˆ 23k events/s
    å³°å€¼ï¼š100kâ€“200k events/s

Metadata operationsï¼ˆå¦‚ rename/moveï¼‰ï¼š

-   5M ops/day â†’ \~60 ops/sec ï¼ˆå³°å€¼ 1k--3kï¼‰

## 3.3 Storage Estimation

å‡è¨­ï¼š

-   500M users
-   å¹³å‡æ¯äºº 10GB

```
    500M * 10GB = 5 exabytes (raw)
```

é€é chunk-level dedupï¼š

-   å„²å­˜é‡å¯ä¸‹é™ 50--90%
-   Network å‚³è¼¸ä¹Ÿå¤§é‡æ¸›å°‘

------------------------------------------------------------------------

# 4. ç³»çµ±æ¶æ§‹ï¼ˆSingle-Region Baselineï¼‰

### Core Components

1.  **Metadata Serviceï¼ˆstrong consistencyï¼‰**\
2.  **Metadata DBï¼ˆSQL sharded / NoSQLï¼‰**\
3.  **Object Storageï¼ˆchunk-based, immutableï¼‰**\
4.  **Sync Change Journal**\
5.  **Sync Serviceï¼ˆå¢é‡åŒæ­¥ï¼‰**\
6.  **Notification Serviceï¼ˆpush eventsï¼‰**\
7.  **Client Sync Agentï¼ˆlocal diff engineï¼‰**

------------------------------------------------------------------------

# 5. Component Deep Dive

## 5.1 Chunking & Dedupï¼ˆæ ¸å¿ƒï¼‰

ä½¿ç”¨å›ºå®šé•·åº¦ï¼ˆ4MBï¼‰æˆ– Rabin Fingerprint å¯è®Šé•· chunkã€‚

### å„ªé»ï¼š

-   æ¸›å°‘ä¸Šå‚³é‡ï¼ˆåªä¸Šå‚³æ–°å¢ chunksï¼‰\
-   Global dedupï¼ˆè·¨ä½¿ç”¨è€…å…±äº«ç›¸åŒ chunksï¼‰\
-   æ”¯æ´ resumable upload

### Chunk Metadata

    chunk_id (hash)
    size
    checksum
    physical location

### File Version Metadata

    file_id
    list_of_chunks
    mtime
    etag
    version

------------------------------------------------------------------------

## 5.2 Metadata Serviceï¼ˆå¼·ä¸€è‡´ä¸»é«”ï¼‰

ç®¡ç†ï¼š

-   folder treeï¼ˆinode çµæ§‹ï¼‰\
-   file attributes\
-   version history\
-   ACL / sharing metadata

Rename / move åƒ…ä¿®æ”¹ metadata pointer â†’ O(1)

Shardingï¼š

-   ä¾ user_id æˆ– namespace åˆ†ç‰‡\
-   å¤§ç”¨æˆ¶ï¼ˆå¤§é‡æª”æ¡ˆï¼‰å¯å–®ç¨åˆ†ç‰‡

------------------------------------------------------------------------

## 5.3 Sync Change Journalï¼ˆDropbox çš„éˆé­‚ï¼‰

æ¯å€‹ user æœ‰ï¼š

    seq_id: monotonic increasing integer
    entries: (seq_id, file_id, operation, version)

è£ç½®åŒæ­¥ï¼š

    client â†’ server: give me changes since seq = X
    server â†’ client: list of changes

ç‰¹æ€§ï¼š

-   å¢é‡åŒæ­¥ï¼ˆä¸éœ€å‚³æ•´æ£µ folder treeï¼‰\
-   crash-safe\
-   conflict detection

------------------------------------------------------------------------

## 5.4 Notification Service

è² è²¬æ¨æ’­ "æœ‰æ›´æ–°" äº‹ä»¶ã€‚

æŠ€è¡“é¸é …ï¼š

-   WebSocket\
-   Long polling\
-   Pub/Subï¼ˆKafka / Redis Streamï¼‰

------------------------------------------------------------------------

# 6. æ ¸å¿ƒæµç¨‹

## 6.1 Upload File Workflow

1.  Client åµæ¸¬æª”æ¡ˆè®Šæ›´ï¼ˆlocal FS watcherï¼‰\
2.  åˆ‡ chunk + è¨ˆç®— hash\
3.  å• server å“ªäº› chunks å·²å­˜åœ¨\
4.  ä¸Šå‚³ missing chunks\
5.  æ›´æ–° metadata\
6.  journal append\
7.  push é€šçŸ¥å…¶ä»–è£ç½®

------------------------------------------------------------------------

## 6.2 Multi-device Sync Workflow

1.  Client å•Ÿå‹•\
2.  é€å‡º last seen seq\
3.  Server å›å‚³å¢é‡ changes\
4.  Client å¥—ç”¨è®Šæ›´\
5.  è‹¥ç™¼ç”Ÿ conflict â†’ å»ºç«‹ "conflicted copy"

------------------------------------------------------------------------

## 6.3 Sharing Workflow

-   public linkï¼šanonymous object fetch\
-   permission-based shareï¼šfolder pointer share\
-   shared folder = shared metadata namespace

------------------------------------------------------------------------

# 7. Multi-Region Architectureï¼ˆé€²éšï¼‰

------------------------------------------------------------------------

## 7.1 Why multi-region is complicated?

å› ç‚ºï¼š

-   metadata **éœ€è¦å¼·ä¸€è‡´**ï¼Œè·¨ region consensus æˆæœ¬æ¥µé«˜\
-   chunk storage **å¯ eventual**ï¼ˆimmutableï¼‰\
-   sync journal éœ€è¦æŒ‰é †åºã€ä¸èƒ½äº‚åº

------------------------------------------------------------------------

## 7.2 Architecture Pattern Aï¼šGlobal Strong Consistencyï¼ˆGoogle Drive Styleï¼‰

ä½¿ç”¨ï¼š

-   Spanner\
-   TrueTime API\
-   globally ordered metadata updates

å„ªé»ï¼š

-   ç„¡äº‚åºå•é¡Œ\
-   sharingã€renameã€move éå¸¸ä¹¾æ·¨

ç¼ºé»ï¼š

-   è²´\
-   å¯«å…¥å»¶é²ç•¥é«˜

------------------------------------------------------------------------

## 7.3 Architecture Pattern Bï¼ˆæ›´æ™®éï¼‰ï¼š**Per-user Primary Region**

æ¯å€‹ user çš„ metadataï¼š

-   åªä½æ–¼ä¸€å€‹ primary regionï¼ˆå¦‚ US-Eastï¼‰\
-   æ‰€æœ‰ sync éƒ½å¾€ primary region é€\
-   è‹¥ user ç§»å‹•åœ°ç†ä½ç½® â†’ å¯ migrate primary region

### å„ªé»ï¼š

-   é¿å… global coordination\
-   metadata consistency ä¸æœƒäº‚\
-   cross-user sharing æ™‚æ‰éœ€è¦è·¨å€æºé€š

------------------------------------------------------------------------

## 7.4 Chunk Storage Multi-Region

Chunks å±¬ immutableï¼š

-   å¯åœ¨ local region å„²å­˜å‰¯æœ¬\
-   ç”¨ CDN åŠ é€Ÿä¸‹è¼‰\
-   æ›´æ”¹ chunk referencesï¼ˆmetadataï¼‰å°±èƒ½åŒæ­¥ç‰ˆæœ¬

------------------------------------------------------------------------

# 8. Scalability Engineering

## Metadata Scaling

-   Sharding by user\
-   Using SQLï¼ˆMySQL shardï¼‰æˆ– NoSQLï¼ˆFoundationDB / Cassandraï¼‰\
-   Folder caching

## Sync Journal Scaling

-   append-only log\
-   separate per-user shard\
-   queries independent

## Object Storage Scaling

-   chunk immutable\
-   S3-like storage\
-   low-cost replication

------------------------------------------------------------------------

# 9. Failure Scenarios & Recovery

### Metadata DB crash

-   replicate + consensus\
-   WAL-based recovery

### Journal corruption

-   checksum\
-   shadow log\
-   snapshot based recovery

### Chunk storage outage

-   fallback to replica\
-   long-term: erasure coding

------------------------------------------------------------------------

# 10. Follow-up Questions + æ¨™æº–å›ç­”

------------------------------------------------------------------------

### Q1. å¦‚ä½•é¿å… metadata hot partitionï¼Ÿ

**Aï¼š**

-   shard by user\
-   large users åˆ†è£‚ shard\
-   cache frequently accessed directory listing\
-   batch operations

------------------------------------------------------------------------

### Q2. å¦‚ä½•è™•ç†å¤§é‡å°æª”æ¡ˆï¼ˆ100k+ filesï¼‰ï¼Ÿ

**Aï¼š**

-   batch sync events\
-   compact directory representation\
-   Bloom filter avoid unnecessary fetch\
-   reduce metadata calls

------------------------------------------------------------------------

### Q3. å¦‚ä½•æ”¯æ´ offline editingï¼Ÿ

**Aï¼š**

-   local delta log\
-   conflict detection with version vector\
-   reconcile on reconnect

------------------------------------------------------------------------

### Q4. rename / move ç‚ºä½•æ˜¯ O(1)ï¼Ÿ

**Aï¼š**

-   metadata pointer update\
-   chunk ä¸è®Š\
-   no physical movement

------------------------------------------------------------------------

### Q5. å¦‚ä½•è™•ç† 1GB+ å¤§æª”æ¡ˆï¼Ÿ

**Aï¼š**

-   multipart upload\
-   chunk resume\
-   speculative parallel upload

------------------------------------------------------------------------

### Q6. å¦‚ä½•é”æˆ global dedupï¼Ÿ

**Aï¼š**

-   chunk hash\
-   index table for chunk store\
-   salted hash to prevent privacy leak

------------------------------------------------------------------------

### Q7. multi-region conflict å¦‚ä½•è™•ç†ï¼Ÿ

**Aï¼š**

-   per-user region â†’ avoid most conflict\
-   shared folder uses version vector or timestamp ordering\
-   if conflict â†’ auto create conflict copy

------------------------------------------------------------------------

# 11. PlantUMLï¼ˆHugo Shortcodeï¼‰

{{< plantuml >}} 
@startuml 
actor Client
Client --> "Sync Service" : upload/download diff 
Client --> "Notification Service" : subscribe
"Sync Service" --> "Metadata Service" : update metadata 
"Metadata Service" --> "Metadata DB" : read/write
"Sync Service" --> "Object Storage" : upload chunks 
"Metadata Service" --> "Change Journal" : append changes "Notification Service" --> Client : notify updates

@enduml 
{{< /plantuml >}}

---
## 2. File Storage Serviceï¼ˆDropbox é¡ç³»çµ±ï¼‰

### 2.1 é¡Œç›®é‡è¿°èˆ‡å‡è¨­

- é¡Œç›®ï¼šè¨­è¨ˆé¡ Dropbox / Google Drive çš„é›²ç«¯æª”æ¡ˆå„²å­˜èˆ‡åŒæ­¥æœå‹™ã€‚  
- åŠŸèƒ½éœ€æ±‚ï¼š  
  - ä¸Šå‚³ / ä¸‹è¼‰æª”æ¡ˆ  
  - å¤šè£ç½®åŒæ­¥ï¼ˆæ¡Œæ©Ÿ / æ‰‹æ©Ÿï¼‰  
  - ç‰ˆæœ¬æ­·å²ï¼ˆå¯ä»¥å›å¾©èˆŠç‰ˆæœ¬ï¼‰  
  - åˆ†äº«é€£çµ / æ¬Šé™æ§åˆ¶ï¼ˆé€²éšï¼‰  
- éåŠŸèƒ½éœ€æ±‚ï¼š  
  - å¯å„²å­˜å¤§é‡æª”æ¡ˆï¼ˆPB ç´šï¼‰  
  - é«˜è€ä¹…æ€§ï¼ˆ11 å€‹ 9 ç­‰ç´šï¼‰  
  - è·¨ device eventual consistency å¯æ¥å—  
  - ä¸Šå‚³ / ä¸‹è¼‰ throughput ä»¥åŠæ–·ç·šæ¢å¾©  

### 2.2 é«˜éšæ¶æ§‹èªªæ˜

- Client æœƒæŠŠæª”æ¡ˆåˆ‡æˆ fixed æˆ– variable-size chunksï¼Œä¸Šå‚³è‡³ Storage nodesã€‚  
- Metadata service ç®¡ç†ï¼šfolder çµæ§‹ã€æª”æ¡ˆç‰ˆæœ¬ã€æ¯å€‹ç‰ˆæœ¬å°æ‡‰çš„ chunk åˆ—è¡¨ã€‚  
- å¯«å…¥ï¼š  
  - Client â†’ Upload Serviceï¼šä¸Šå‚³ chunksï¼Œè¨ˆç®— hash åš dedupã€‚  
  - å¯«å…¥ Object Storeï¼ˆå¦‚ S3 / HDFS / è‡ªå»ºï¼‰ã€‚  
  - Metadata Service ç´€éŒ„ file â†’ [chunk hashes]ã€‚  
- è®€å–ï¼š  
  - Client å‘ Metadata Service æŸ¥è©¢æŸæª”æ¡ˆç‰ˆæœ¬çš„ chunk åˆ—è¡¨ï¼Œå†å¾ Storage æ‹¿ chunksï¼Œçµ„åˆæˆæª”æ¡ˆã€‚  
- Syncï¼š  
  - Client æœ‰ local indexï¼Œå®šæœŸèˆ‡ Metadata Service æ¯”è¼ƒå·®ç•°ï¼Œé€²è¡Œå¢é‡åŒæ­¥ã€‚  

### 2.3 PlantUML

{{< plantuml >}}
@startuml
title File Storage Service - High Level Architecture

actor User
rectangle "Client (Sync Agent)" as CLIENT

rectangle "API Gateway" as API
rectangle "Metadata Service" as META
database "Metadata DB (files, versions, chunks)" as METADB

rectangle "Upload Service" as UP
rectangle "Download Service" as DOWN
cloud "Object Storage (Chunks, immutable)" as STORE

queue "Change Log (Events)" as CHANGELOG

User --> CLIENT : file changes
CLIENT --> API : upload/download requests

API --> UP : upload file
UP --> META : get/alloc file_id
UP --> STORE : upload chunks
UP --> META : write file metadata
(path, version, chunk list)
META --> METADB

API --> DOWN : download file
DOWN --> META : get file metadata
META --> METADB
DOWN --> STORE : fetch chunks
STORE --> DOWN
DOWN --> CLIENT : file data

META --> CHANGELOG : file change events
CHANGELOG --> CLIENT : sync updates

@enduml
{{< /plantuml >}}

### 2.4 å£é ­è¬›ç¨¿ï¼ˆç´„ 2â€“3 åˆ†é˜ï¼‰

> æˆ‘æŠŠé›²ç«¯ç¡¬ç¢Ÿåˆ†æˆå…©å¡Šï¼šä¸€å€‹æ˜¯ã€Œå·¨å¤§è€Œä¾¿å®œçš„ç‰©ç†å„²å­˜ï¼ˆObject Storageï¼‰ã€ï¼Œå¦ä¸€å€‹æ˜¯ã€Œæ¯”è¼ƒå°ä½†å¾ˆé—œéµçš„ Metadata Serviceã€ã€‚  
> <br>
> ç•¶ä½¿ç”¨è€…ä¸Šå‚³æª”æ¡ˆæ™‚ï¼ŒClient æœƒå…ˆæŠŠæª”æ¡ˆåˆ‡æˆ chunksï¼Œè¨ˆç®—æ¯å€‹ chunk çš„ hashï¼Œé¿å…é‡è¤‡ä¸Šå‚³ç›¸åŒå…§å®¹ã€‚ä¸Šå‚³æµç¨‹é€é Upload Service æŠŠ chunks å¯«é€² Object Storeï¼Œä¸¦ä¸”åœ¨ Metadata Service è¨»å†Šä¸€å€‹æ–°çš„æª”æ¡ˆç‰ˆæœ¬ï¼Œç´€éŒ„é€™å€‹ç‰ˆæœ¬ç”¨åˆ°å“ªäº› chunksã€‚é€™æ¨£å¯ä»¥åœ¨ä½¿ç”¨è€…æ”¹å‹•éƒ¨åˆ†å…§å®¹æ™‚ï¼Œåªé‡æ–°ä¸Šå‚³è®Šå‹•çš„ chunksã€‚  
> <br>
> ä¸‹è¼‰å‰‡æ˜¯å…ˆå• Metadata Service æ‹¿åˆ°æŸå€‹æª”æ¡ˆç‰ˆæœ¬å°æ‡‰çš„ chunk åˆ—è¡¨ï¼Œå†å‘ Object Store æŠ“å–ä¸¦åœ¨ Client ç«¯çµ„è£ã€‚  
> <br>
> å¤šè£ç½®åŒæ­¥ä¾è³´ Change Logï¼šMetadata Service å°‡æ¯å€‹æª”æ¡ˆè®Šå‹•å¯«é€²ä¸€å€‹ log streamï¼ŒClient ç«¯æœƒè¿½é€™å€‹ logï¼Œçœ‹åˆ°æœ‰æ–°çš„ç‰ˆæœ¬æˆ–åˆªé™¤å‹•ä½œå°±åŒæ­¥æœ¬åœ°è³‡æ–™å¤¾ã€‚è¡çªæ™‚å¯ä»¥ä¾ timestamp æˆ–ç”¨ three-way merge ç­–ç•¥è™•ç†ã€‚  

---
